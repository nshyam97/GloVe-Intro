{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Example\n",
    "\n",
    "The following notebook is using [this](https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db) tutorial as a start into the world of GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a pretrained vector provided by the GloVe team and is available on their [website](https://nlp.stanford.edu/projects/glove/). These text files are nicely formatted as they contain single a single word followed by *N* numbers. The *N* numbers that follow this word represent a point in *N* dimensional space. Numbers which are close to each other are close synonyms. The higher the dimension, the better the accuracy but more computation is needed. I will be using an *N* of 50 for this project to mimic the tutorial that this is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I will create an embeddings dictionary for word embedding. \n",
    "# Word embedding involves mapping words or phrases to vectors or numbers\n",
    "embeddings_dict = {}\n",
    "# Open the file with 50 dimensions\n",
    "with open('glove/glove.6B.50d.txt', 'r') as f:\n",
    "    # For every line, which is every word in this file\n",
    "    for line in f:\n",
    "        # Split the line up using default separator which is whitespace\n",
    "        values = line.split()\n",
    "        # The first item in the line is the word so assign that\n",
    "        word = values[0]\n",
    "        # Every other value in the line is a vector position\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        # Now map the word to the vectors that we have got\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes all the words we have an stores them with their respective vector positions using the embeddings_dict dictionary. We can now use this dictionary to do some cool things with it.\n",
    "\n",
    "Below is a function that will take in a word and find other words that are similar to it using the vector positioning I talked about earlier. What this function does is take the word that was inputted and uses the dictionary keys, which are the words we embedded above, and find the distance between all these words and the word inputted. Instead of returning an alphabetically sorted list, we use the distance we calculated as the sorting key so that we get the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: \n",
    "                  spatial.distance.euclidean(embeddings_dict[word], embedding))[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shoes', 'handbag', 'underwear', 'sneakers', 'leather']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(embeddings_dict['shoe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as an example, I used 'shoe' as the word to test and outputted the top 5 most similar words in our dictionary. As you can see the first word, which is supposedly the most similar is 'shoes'. Makes sense! The other 4 words are all items of clothing which shows that these pretrained GloVe vectors do not just go on the words that are the most similar but also the meaning that is most similar. An interesting thing to note is that 'sneakers', a type of shoe, is 4th in the list behind handbag and underwear.\n",
    "\n",
    "What we can do is adapt the above function to show us the euclidean distances between the top 5 words and the word we want. We can use these distances to see how 'far' apart the words are from 'shoe'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance_of_embeddings(embedding):\n",
    "    words = find_closest_embeddings(embedding)\n",
    "    distances = list(map(lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding), words))\n",
    "    return dict(zip(words, distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shoes': 2.918541431427002,\n",
       " 'handbag': 3.0618388652801514,\n",
       " 'underwear': 3.096071481704712,\n",
       " 'sneakers': 3.323274850845337,\n",
       " 'leather': 3.34987211227417}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_distance_of_embeddings(embeddings_dict['shoe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 'shoes' is clearly the most similar which makes sense. We can also see the distance between 'shoe' and 'handbag' and 'underwear' are both very similar but sneakers is bit further away from these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit9e4073255a9e4e14bafa28b8aba51016"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
